from datetime import datetime, timedelta
import json
import time

import httpx
from curl2json.parser import parse_curl
import redis

from kafka import KafkaProducer, KafkaConsumer

from config import config 

# ETL for crawler DAG
def extract_tokens(**kwargs):
    BLOOM_KEY = config["redis_bloom_filter"]  
    rdb = redis.Redis(host=config["redis_host"], port=config["redis_port"]) 
    
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Bloom filter
    if not rdb.exists(BLOOM_KEY):
        try:
            rdb.execute_command("BF.RESERVE", BLOOM_KEY, 0.05, 1_000_000)
            print(f"âœ… Bloom filter Ø¨Ø§ Ù†Ø§Ù… {BLOOM_KEY} Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯")
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Bloom filter: {e}")
    else:
        print(f"âœ… Bloom filter Ø¨Ø§ Ù†Ø§Ù… {BLOOM_KEY} ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯")

    curl_command = """curl 'https://api.divar.ir/v8/postlist/w/search' \
      --compressed \
      -X POST \
      --data-raw '{"city_ids":["1"],"pagination_data":{"@type":"type.googleapis.com/post_list.PaginationData","page":0,"layer_page":0,"search_bookmark_info":{"alert_state":{}}},"search_data":{"form_data":{"data":{"category":{"str":{"value":"apartment-sell"}}}},"server_payload":{"@type":"type.googleapis.com/widgets.SearchData.ServerPayload","additional_form_data":{"data":{"sort":{"str":{"value":"sort_date"}}}}}}}'"""
    
    parsed_curl = parse_curl(curl_command)
    parsed_curl.pop("cookies", None)
    
    client_params = {
        "verify": True,
        "headers": parsed_curl.pop("headers", {}),
    }
    
    all_tokens = set()
    max_pages = 100
    
    with httpx.Client(**client_params) as client:
        # GET for get Cookies
        try:
            resp = client.get("https://divar.ir")
            resp.raise_for_status()
            print("âœ… Cookies Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú¯Ø±ÙØªÙ† cookies: {e}")
            return

        curl_data = json.loads(parsed_curl.get("data"))

        for page in range(max_pages):
            try:
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ pagination_data Ø¨Ø±Ø§ÛŒ ØµÙØ­Ù‡ ÙØ¹Ù„ÛŒ
                curl_data["pagination_data"]["page"] = page
                curl_data["pagination_data"]["layer_page"] = 0
                parsed_curl["data"] = json.dumps(curl_data)
                
                # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª POST
                response = client.request(
                    method=parsed_curl.get("method", "POST"),
                    url=parsed_curl["url"],
                    headers=parsed_curl.get("headers", {}),
                    content=parsed_curl.get("data"),
                    params=parsed_curl.get("params")
                )
                response.raise_for_status()
                result = response.json()
            
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
                widgets = result.get("list_widgets", []) or []
                tokens = [w.get("data", {}).get("token") for w in widgets if w.get("data", {}).get("token")]
                if not tokens:
                    print(f"â›”ï¸ ØµÙØ­Ù‡ {page}: Ù‡ÛŒÚ† ØªÙˆÚ©Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ ØªÙˆÙ‚Ù.")
                    break
                
                # for t in tokens:
                #     print(f"ğŸ”¹ ØªÙˆÚ©Ù† ÛŒØ§ÙØª Ø´Ø¯: {t}")
                
                # print(f"ğŸ“„ ØµÙØ­Ù‡ {page}: {result.get('list_widgets')[0].get('data').get('title')}")
                print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§: {len(widgets)}")
                print(f" ØµÙØ­Ù‡ : {page}")
                      
                # for w in widgets:
                #     print(f"ğŸ”¹ ØªÙˆÚ©Ù† ÛŒØ§ÙØª Ø´Ø¯: {w}")

                # Ú†Ú© Ú©Ø±Ø¯Ù† ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø§ Bloom filter
                duplicate_count, new_tokens = 0, []
                for token in tokens:
                    exists = rdb.execute_command("BF.EXISTS", BLOOM_KEY, token)
                    if exists:
                        duplicate_count += 1
                    else:
                        new_tokens.append(token)
                        rdb.execute_command("BF.ADD", BLOOM_KEY, token)

                all_tokens.update(new_tokens)
                ratio = duplicate_count / len(tokens) if tokens else 1
                print(f"ğŸ“Š ØµÙØ­Ù‡ {page}: {duplicate_count}/{len(tokens)} ØªÚ©Ø±Ø§Ø±ÛŒ ({ratio:.0%})")
            
                if ratio >= 0.3:
                    print(f"ğŸ›‘ ØµÙØ­Ù‡ {page}: Ø¨ÛŒØ´ Ø§Ø² 30Ø¯Ø±ØµØ¯ ØªÚ©Ø±Ø§Ø±ÛŒ â€” ØªÙˆÙ‚Ù.")
                    break
            
                # update pagination_data 
                pagination_info = result.get("pagination", {}) or {}
                curl_data["pagination_data"] = pagination_info.get("data", curl_data["pagination_data"])
                
                time.sleep(1.5)   

            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØµÙØ­Ù‡ {page}: {e}")
                break


    kwargs["ti"].xcom_push(key="extracted_tokens", value=list(all_tokens))
    print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯ â€” {len(all_tokens)} ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø¨Ù‡ XCom.")
    
def filter_tokens(**kwargs):
    tokens = kwargs['ti'].xcom_pull(key='extracted_tokens', task_ids='extract_tokens') or []
    if not tokens:
        print("Ù‡ÛŒÚ† ØªÙˆÚ©Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        kwargs['ti'].xcom_push(key='filtered_tokens', value=[])
        return

    # r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT)
    # new_tokens = []
    # for token in tokens:
    #     exists = r.execute_command("BF.EXISTS", REDIS_BLOOM_FILTER, token)
    #     if not exists:
    #         r.execute_command("BF.ADD", REDIS_BLOOM_FILTER, token)
    #         new_tokens.append(token)

    kwargs['ti'].xcom_push(key='filtered_tokens', value=tokens)
    print(f"Ø§Ù†ØªÙ‚Ø§Ù„ ÛŒØ§ÙØª: {len(tokens)} ØªÙˆÚ©Ù† Ø¨Ù‡ XCom")

def produce_to_kafka(**kwargs):
    tokens = kwargs['ti'].xcom_pull(key='filtered_tokens', task_ids='filter_tokens')
    if not tokens:
        print("Ù‡ÛŒÚ† ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ú©Ø§ÙÚ©Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return

    producer = KafkaProducer(
        bootstrap_servers=config["kafka_bootstrap_servers"],
        value_serializer=lambda v: json.dumps(v).encode("utf-8"),
    )
    for token in tokens:
        producer.send(config["kafka_topic"], token)
    producer.flush()
    print(f"Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {len(tokens)} ØªÙˆÚ©Ù† Ø¨Ù‡ Ú©Ø§ÙÚ©Ø§")
