from datetime import datetime, timedelta
import json
import re
import time
from collections import deque
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup
from urllib.robotparser import RobotFileParser
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.base import BaseSensorOperator
from airflow.utils.decorators import apply_defaults
import redis
from pymongo import MongoClient
from pymongo.errors import DuplicateKeyError
from kafka import KafkaProducer, KafkaConsumer

from curl2json.parser import parse_curl
from redisbloom.client import Client as RedisBloom

# crawler
USER_AGENT_DEFAULT = "DivarTokenCrawler/1.0 (+https://example.com)"
FOLLOW_PATH_KEYWORDS = ("/s/tehran/buy-apartment", "page=")

# Redis
REDIS_HOST = "172.16.36.111"
REDIS_PORT = 6379
REDIS_BLOOM_FILTER = "divar_tokens_bloom_8"

# Kafka
KAFKA_BOOTSTRAP_SERVERS = ["172.16.36.111:9092"]
KAFKA_TOPIC = "divar_tokens8"

# MongoDB
MONGO_URI = "mongodb://appuser:appassword@172.16.36.111:27017/delta-datasets"
MONGO_DB = "delta-datasets"
MONGO_COLLECTION = "crawl.5"

# API endpoint
DIVAR_API_URL = "https://api.divar.ir/v8/posts-v2/web/{}"

# --- ØªÙˆØ§Ø¨Ø¹ ETL Ø¨Ø±Ø§ÛŒ DAG ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ù‡ ---
def extract_tokens(**kwargs):
    BLOOM_KEY = REDIS_BLOOM_FILTER  
    rdb = redis.Redis(host=REDIS_HOST, port=REDIS_PORT)  # REDIS_HOST Ø§Ø² ØªÙ†Ø¸ÛŒÙ…Ø§Øª
    
    try:
        rdb.execute_command("BF.RESERVE", BLOOM_KEY, 0.1, 1_000_000)
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Bloom filter: {e}")

    # first cURL command 
    curl_command = """curl 'https://api.divar.ir/v8/postlist/w/search' \
      --compressed \
      -X POST \
      -H 'User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:143.0) Gecko/20100101 Firefox/143.0' \
      -H 'Accept: application/json, text/plain, */*' \
      -H 'Accept-Language: en-US,en;q=0.5' \
      -H 'Accept-Encoding: gzip, deflate, br, zstd' \
      -H 'Content-Type: application/json' \
      -H 'Referer: https://divar.ir/' \
      -H 'X-Screen-Size: 1920x389' \
      -H 'X-Standard-Divar-Error: true' \
      -H 'X-Render-Type: CSR' \
      -H 'traceparent: 00-963166ebc6e862920179136b175a7c0e-a16aa7f879154079-00' \
      -H 'Origin: https://divar.ir' \
      -H 'Sec-Fetch-Dest: empty' \
      -H 'Sec-Fetch-Mode: cors' \
      -H 'Sec-Fetch-Site: same-site' \
      -H 'Authorization: Basic eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzaWQiOiI2ODg1MTgxNi00NDc4LTRhNmYtODRhMi03YzI5ZjMwMjc2NWMiLCJ1aWQiOiI2ZmNlNjkxYi04MmI5LTRlMTMtODc3ZS1lOTFjOGJlYWNhMWUiLCJ1c2VyIjoiMDkyMDUyMDI0MDAiLCJ2ZXJpZmllZF90aW1lIjoxNzU5MjM0NDA5LCJpc3MiOiJhdXRoIiwidXNlci10eXBlIjoicGVyc29uYWwiLCJ1c2VyLXR5cGUtZmEiOiLZvtmG2YQg2LTYrti124wiLCJleHAiOjE3NjE4MjY0MDksImlhdCI6MTc1OTIzNDQwOX0.KSxXkAOtRDCzr5n_ipKtsraMApOy_edTwksvU2k7GLY' \
      -H 'Connection: keep-alive' \
      -H 'Cookie: did=5511a5a2-2db4-425f-a27a-1818418ba676; cdid=3b14eaba-403f-4d2c-9ee1-07b203822758; _gcl_au=1.1.1647320282.1754234832; theme=dark; _ga_1G1K17N77F=GS2.1.s1760002957$o12$g1$t1760003184$j54$l0$h0; _ga=GA1.1.1799044096.1754234832; multi-city=tehran%7C; city=tehran; _clck=1d3brth%5E2%5Efzr%5E0%5E2041; player_id=7c8b83ef-d5c7-46ef-9021-0bb889491ba2; disable_map_view=true; referrer=undefined; csid=9f7fa3f89e03351903; token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzaWQiOiI2ODg1MTgxNi00NDc4LTRhNmYtODRhMi03YzI5ZjMwMjc2NWMiLCJ1aWQiOiI6ZmNlNjkxYi04MmI5LTRlMTMtODc3ZS1lOTFjOGJlYWNhMWUiLCJ1c2VyIjoiMDkyMDUyMDI0MDAiLCJ2ZXJpZmllZF90aW1lIjoxNzU5MjM0NDA5LCJpc3MiOiJhdXRoIiwidXNlci10eXBlIjoicGVyc29uYWwiLCJ1c2VyLXR5cGUtZmEiOiLZvtmG2YQg2LTYrti124wiLCJleHAiOjE3NjE4MjY0MDksImlhdCI6MTc1OTIzNDQwOX0.KSxXkAOtRDCzr5n_ipKtsraMApOy_edTwksvU2k7GLY; ff=%7B%22f%22%3A%7B%22custom_404_experiment%22%3Atrue%2C%22web_show_ios_appstore_promotion_banner%22%3Atrue%2C%22foreigner_payment_enabled%22%3Atrue%2C%22disable_recommendation%22%3Atrue%2C%22shopping_assistant_in_prediction_enabled%22%3Atrue%2C%22enable_filter_post_count_web%22%3Atrue%2C%22enable_non_lazy_image_post_card%22%3Atrue%7D%2C%22e%22%3A1760006554790%2C%22r%22%3A1760089354790%7D' \
      -H 'TE: trailers' \
      --data-raw '{"city_ids":["1"],"pagination_data":{"@type":"type.googleapis.com/post_list.PaginationData","last_post_date":"2025-10-09T09:41:17.567588Z","page":0,"layer_page":0,"search_uid":"352ad3e9-9021-414e-992e-f6edc366a03f","cumulative_widgets_count":74,"viewed_tokens":"H4sIAAAAAAAE/yySy5KyQAyFXyhTxSgKLiOMXGylRUYuGyv+XrCVVscB1Kf/Kzi71Ely+HJopIW71PEJkHQSyCIApEqUVvkCpOP6uPAlIFXSu5YtIO38vRGuAOma6iZPeWZZ27sKkFRiX8czQHIsP9sO2FDm5egISCSTbTLulKAXJl1hzg91V1xGtzuv99P9ctopgV0hIJ2anQiIeayhuv8DpLlK6uDWzVz8xmUe5b5GD0AKdC+fFYBkPbeX4odntO4/+JzolR5jD5B+w/bRPrklc8diRcmAbhkrKvtes4+WeRudASmvJ0U45Ja8yA41Up+2KgEp/LC9Yt21glAeGF5sDPfvwPss5JYwjaJDdXOn4jBXm8X1agGSJ6Kvr/elZmmYfGCWnAXHcmqansHO5E5Xmnk8fY/rmA2lufy038Vt3ADSWcR7m4PSVqpszue0qzSxspFDJ9aAVJI9f/b4F/hOm7KhFqYU/AktgnztANLMj0kab+eUdoBUZMn522cwH2uHCbVvGhWnoX7C/IMxzOkEBxN2trJsLt7rUcxbi/3hvlh2SpB4/CREMmvltlPMKPt7G+NBH5AqdRn93v4HAAD//1ag71+HAgAA"},"disable_recommendation":false,"map_state":{"camera_info":{"bbox":{}}},"search_data":{"form_data":{"data":{"category":{"str":{"value":"apartment-sell"}}}},"server_payload":{"@type":"type.googleapis.com/widgets.SearchData.ServerPayload","additional_form_data":{"data":{"sort":{"str":{"value":"sort_date"}}}}}}}'"""

    # ØªØ¨Ø¯ÛŒÙ„ cURL Ø¨Ù‡ JSON
    parsed_curl = parse_curl(curl_command)
    
    # Ø­Ø°Ù Ú©Ù„ÛŒØ¯ verify Ú©Ù‡ httpx.Client.request Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù‡
    parsed_curl.pop("verify", None)
    
    # Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† SSL verification Ø¯Ø§Ø±ÛŒØŒ Ø¯Ø± Ú©Ù„Ø§ÛŒÙ†Øª ØªÙ†Ø¸ÛŒÙ… Ú©Ù†
    # client = httpx.Client(timeout=15, verify=True)  # Ø¨Ø±Ø§ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† SSL: verify=False
    client = httpx.Client(timeout=15, verify=False)  # ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† SSL verification
    
    all_tokens = set()
    current_page = 0
    max_pages = 100  
    # last_post_date = None

    # ğŸ§­ Ù…Ù‚Ø§Ø¯ÛŒØ± ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
    last_post_date = None
    search_uid = None
    cumulative_widgets_count = None
    viewed_tokens = None
    
    try:
        next_page = 0
        page_counter = 0

        while next_page is not None and page_counter <= max_pages:
            print(f"ğŸ“„ Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ {page_counter} Ø§Ø² API Ø¯ÛŒÙˆØ§Ø± ...")

            curl_data = json.loads(parsed_curl.get("data"))

            # Ø§Ú¯Ø± Ø§Ø² ØµÙØ­Ù‡ Ù‚Ø¨Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª pagination Ø¯Ø§Ø±ÛŒØŒ Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†
            if last_post_date:
                curl_data["pagination_data"]["last_post_date"] = last_post_date
            if search_uid:
                curl_data["pagination_data"]["search_uid"] = search_uid
            if cumulative_widgets_count:
                curl_data["pagination_data"]["cumulative_widgets_count"] = cumulative_widgets_count
            if viewed_tokens:
                curl_data["pagination_data"]["viewed_tokens"] = viewed_tokens

            curl_data["pagination_data"]["page"] = next_page
            curl_data["pagination_data"]["layer_page"] = next_page

            parsed_curl["data"] = json.dumps(curl_data)

            try:
                resp = client.request(**parsed_curl)
                resp.raise_for_status()
                data = resp.json()
            except httpx.RequestError as e:
                print(f"âŒ Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¯Ø± ØµÙØ­Ù‡ {page_counter}: {e}")
                break
            except httpx.HTTPStatusError as e:
                print(f"âŒ Ø®Ø·Ø§ÛŒ HTTP Ø¯Ø± ØµÙØ­Ù‡ {page_counter}: {e.response.status_code}")
                break
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± ØµÙØ­Ù‡ {page_counter}: {e}")
                break

            # ğŸ” Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ pagination Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú†Ù†Ø¯ Ø³Ø§Ø®ØªØ§Ø± Ù…Ù…Ú©Ù†
            if "pagination_data" in data:
                pagination_info = data["pagination_data"]
            elif "pagination" in data:
                pagination_info = data["pagination"]
            elif "seo" in data and "pagination" in data["seo"]:
                pagination_info = data["seo"]["pagination"]
            else:
                pagination_info = {}

            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
            last_post_date = pagination_info.get("last_post_date", last_post_date)
            search_uid = pagination_info.get("search_uid", search_uid)
            cumulative_widgets_count = pagination_info.get("cumulative_widgets_count", cumulative_widgets_count)
            viewed_tokens = pagination_info.get("viewed_tokens", viewed_tokens)
            next_page = pagination_info.get("next_page") or pagination_info.get("page") or None

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
            widgets = data.get("list_widgets", []) or []
            tokens = [w.get("data", {}).get("token") for w in widgets if w.get("data", {}).get("token")]

            if not tokens:
                print("â›”ï¸ Ù‡ÛŒÚ† ØªÙˆÚ©Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ ØªÙˆÙ‚Ù.")
                break

            duplicate_count, new_tokens = 0, []
            for token in tokens:
                exists = rdb.execute_command("BF.EXISTS", BLOOM_KEY, token)
                if exists:
                    duplicate_count += 1
                else:
                    new_tokens.append(token)
                    rdb.execute_command("BF.ADD", BLOOM_KEY, token)

            all_tokens.update(new_tokens)
            ratio = duplicate_count / len(tokens)
            print(f"ğŸ“Š ØµÙØ­Ù‡ {page_counter}: {duplicate_count}/{len(tokens)} ØªÚ©Ø±Ø§Ø±ÛŒ ({ratio:.0%})")
            print(f"ğŸ†• {len(new_tokens)} ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯ Ø§ÙØ²ÙˆØ¯Ù‡ Ø´Ø¯.")
            if last_post_date:
                print(f"ğŸ“… Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® Ù¾Ø³Øª: {last_post_date}")

            if ratio >= 0.3:
                print("ğŸ›‘ Ø¨ÛŒØ´ Ø§Ø² Û³Û°Ùª ØªÚ©Ø±Ø§Ø±ÛŒ â€” ØªÙˆÙ‚Ù.")
                break

            page_counter += 1
            time.sleep(1.5)
    

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
    finally:
        client.close()
        kwargs["ti"].xcom_push(key="extracted_tokens", value=list(all_tokens))
        print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯ â€” {len(all_tokens)} ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ Ø¨Ù‡ XCom.")

def filter_tokens(**kwargs):
    tokens = kwargs['ti'].xcom_pull(key='extracted_tokens', task_ids='extract_tokens') or []
    if not tokens:
        print("Ù‡ÛŒÚ† ØªÙˆÚ©Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        kwargs['ti'].xcom_push(key='filtered_tokens', value=[])
        return

    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT)
    new_tokens = []
    for token in tokens:
        exists = r.execute_command("BF.EXISTS", REDIS_BLOOM_FILTER, token)
        if not exists:
            r.execute_command("BF.ADD", REDIS_BLOOM_FILTER, token)
            new_tokens.append(token)

    kwargs['ti'].xcom_push(key='filtered_tokens', value=new_tokens)
    print(f"ÙÛŒÙ„ØªØ± Ø´Ø¯: {len(new_tokens)} ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯ (Ø§Ø² {len(tokens)})")


def produce_to_kafka(**kwargs):
    tokens = kwargs['ti'].xcom_pull(key='filtered_tokens', task_ids='filter_tokens')
    if not tokens:
        print("Ù‡ÛŒÚ† ØªÙˆÚ©Ù† Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ú©Ø§ÙÚ©Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return

    producer = KafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        value_serializer=lambda v: json.dumps(v).encode("utf-8"),
    )
    for token in tokens:
        producer.send(KAFKA_TOPIC, token)
    producer.flush()
    print(f"Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {len(tokens)} ØªÙˆÚ©Ù† Ø¨Ù‡ Ú©Ø§ÙÚ©Ø§")

# --- ØªÙˆØ§Ø¨Ø¹ ETL Ø¨Ø±Ø§ÛŒ DAG Ù…ØµØ±Ùâ€ŒÚ©Ù†Ù†Ø¯Ù‡ ---
def transform_json_to_doc(data: dict) -> dict:
    doc = {}
    doc["record_timestamp"] = datetime.now().replace(microsecond=0).isoformat(sep=" ")
    doc["cat2_slug"] = data.get("analytics", {}).get("cat2") or "null"
    doc["cat3_slug"] = data.get("analytics", {}).get("cat3") or "null"
    city_data = data.get("city")
    if isinstance(city_data, dict):
        doc["city_slug"] = city_data.get("second_slug", "null")
    else:
        doc["city_slug"] = city_data or "null"
    doc["neighborhood_slug"] = data.get("webengage", {}).get("district") or "null"
    raw_date = data.get("seo", {}).get("unavailable_after")
    doc["created_at_month"] = None
    if raw_date:
        try:
            dt = datetime.strptime(raw_date[:10], "%Y-%m-%d")
            doc["created_at_month"] = dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            pass
    raw_user_type = data.get("webengage", {}).get("business_type")
    mapping = {"personal": "Ø´Ø®ØµÛŒ", "premium-panel": "Ù…Ø´Ø§ÙˆØ± Ø§Ù…Ù„Ø§Ú©"}
    doc["user_type"] = mapping.get(raw_user_type, float("nan"))
    doc["description"] = (
        data.get("seo", {}).get("post_seo_schema", {}).get("description") or "null"
    )
    doc["title"] = data.get("seo", {}).get("web_info", {}).get("title") or "null"
    doc["rent_mode"] = "null"
    doc["rent_value"] = "null"
    doc["rent_to_single"] = "null"
    doc["rent_type"] = "null"
    doc["price_mode"] = "null"
    doc["price_value"] = "null"
    doc["credit_mode"] = "null"
    doc["credit_value"] = "null"
    doc["rent_credit_transform"] = "null"
    doc["transformable_price"] = "null"
    doc["transformable_credit"] = "null"
    doc["transformed_credit"] = "null"
    doc["transformable_rent"] = "null"
    doc["transformed_rent"] = "null"
    list_data = next(
        (s for s in data.get("sections", []) if s.get("section_name") == "LIST_DATA"),
        {},
    )
    widgets = list_data.get("widgets", [])
    breadcrumb = next(
        (s for s in data.get("sections", []) if s.get("section_name") == "BREADCRUMB"),
        {},
    )
    breadcrumb_widget = next(
        (
            w
            for w in breadcrumb.get("widgets", [])
            if w.get("widget_type") == "BREADCRUMB"
        ),
        None,
    )
    current_page_title = (
        breadcrumb_widget.get("data", {}).get("current_page_title", "")
        if breadcrumb_widget
        else ""
    )
    if "Ø±Ø§ÛŒÚ¯Ø§Ù†" in current_page_title or "Ù…Ø¬Ø§Ù†ÛŒ" in current_page_title:
        doc["price_mode"] = "Ù…Ø¬Ø§Ù†ÛŒ"
    elif "ØªÙˆØ§ÙÙ‚ÛŒ" in current_page_title:
        doc["price_mode"] = "ØªÙˆØ§ÙÙ‚ÛŒ"
    elif "Ù…Ù‚Ø·ÙˆØ¹" in current_page_title:
        doc["price_mode"] = "Ù…Ù‚Ø·ÙˆØ¹"
    price_widget = next(
        (
            w
            for w in widgets
            if w.get("widget_type") == "UNEXPANDABLE_ROW"
            and w.get("data", {}).get("title") == "Ù‚ÛŒÙ…Øª Ú©Ù„"
        ),
        None,
    )
    if price_widget:
        value = price_widget.get("data", {}).get("value", "null")
        doc["price_value"] = value.replace(" ØªÙˆÙ…Ø§Ù†", "") if value != "null" else "null"
    physical_fields = [
        "land_size",
        "building_size",
        "deed_type",
        "has_business_deed",
        "floor",
        "rooms_count",
        "total_floors_count",
        "unit_per_floor",
    ]
    for field in physical_fields:
        doc[field] = "null"
    group_feature_row = next(
        (w for w in widgets if w.get("widget_type") == "GROUP_FEATURE_ROW"), None
    )
    modal_features = []
    if group_feature_row:
        modal_features = (
            group_feature_row.get("data", {})
            .get("action", {})
            .get("payload", {})
            .get("modal_page", {})
            .get("widget_list", [])
            or []
        )
    description = next(
        (
            w.get("data", {}).get("text", "")
            for s in data.get("sections", [])
            if s.get("section_name") == "DESCRIPTION"
            for w in s.get("widgets", [])
            if w.get("widget_type") == "DESCRIPTION_ROW"
        ),
        "",
    )
    for widget in widgets:
        if (
            widget.get("widget_type") == "UNEXPANDABLE_ROW"
            and widget.get("data", {}).get("title") == "Ù…ØªØ±Ø§Ú˜ Ø²Ù…ÛŒÙ†"
        ):
            doc["land_size"] = widget.get("data", {}).get("value", "null")
            break
    for widget in widgets:
        if widget.get("widget_type") == "GROUP_INFO_ROW":
            items = widget.get("data", {}).get("items", []) or []
            for item in items:
                title = item.get("title", "")
                value = item.get("value", "")
                if "Ù…ØªØ±Ø§Ú˜" in title:
                    doc["building_size"] = value
                    break
            if doc["building_size"] != "null":
                break
    deed_type_map = {
        "ØªÚ©â€ŒØ¨Ø±Ú¯": "single_page",
        "Ù…Ù†Ú¯ÙˆÙ„Ù‡â€ŒØ¯Ø§Ø±": "single_page",
        "Ù‚ÙˆÙ„â€ŒÙ†Ø§Ù…Ù‡â€ŒØ§ÛŒ": "written_agreement",
        "Ù†Ø§Ù…Ø´Ø®Øµ": "unselect",
        "unselect": "unselect",
        "Ø³Ø§ÛŒØ±": "other",
    }
    for widget in widgets:
        if (
            widget.get("widget_type") == "UNEXPANDABLE_ROW"
            and widget.get("data", {}).get("title") == "Ø³Ù†Ø¯"
        ):
            raw_deed_type = widget.get("data", {}).get("value", None)
            doc["deed_type"] = (
                deed_type_map.get(raw_deed_type, "null") if raw_deed_type else "null"
            )
            break
    else:
        raw_deed_type = next(
            (
                m.get("data", {}).get("value")
                for m in modal_features
                if m.get("data", {}).get("title") == "Ø³Ù†Ø¯"
            ),
            None,
        )
        doc["deed_type"] = (
            deed_type_map.get(raw_deed_type, "null") if raw_deed_type else "null"
        )
    doc["has_business_deed"] = "null"
    floor_map = {"Ù‡Ù…Ú©Ù": "0", "Ù‡Ù…â€ŒÚ©Ù": "0"}
    for widget in widgets:
        if (
            widget.get("widget_type") == "UNEXPANDABLE_ROW"
            and widget.get("data", {}).get("title") == "Ø·Ø¨Ù‚Ù‡"
        ):
            raw_floor = widget.get("data", {}).get("value", "null")
            if raw_floor != "null":
                if raw_floor in floor_map:
                    doc["floor"] = floor_map[raw_floor]
                else:
                    match = re.search(r"(\d+)\s*Ø§Ø²\s*(\d+)", raw_floor)
                    if match:
                        doc["floor"] = match.group(1)
                    else:
                        try:
                            float(raw_floor)
                            doc["floor"] = raw_floor
                        except (ValueError, TypeError):
                            doc["floor"] = "null"
            break
    for widget in widgets:
        if widget.get("widget_type") == "GROUP_INFO_ROW":
            items = widget.get("data", {}).get("items", []) or []
            for item in items:
                title = item.get("title", "")
                value = item.get("value", "")
                if "Ø§ØªØ§Ù‚" in title:
                    doc["rooms_count"] = value
                    break
            if doc["rooms_count"] != "null":
                break
    for widget in widgets:
        if (
            widget.get("widget_type") == "UNEXPANDABLE_ROW"
            and widget.get("data", {}).get("title") == "Ø·Ø¨Ù‚Ù‡"
        ):
            floor_value = widget.get("data", {}).get("value", "null")
            if floor_value != "null":
                match = re.search(r"(\d+)\s*Ø§Ø²\s*(\d+)", floor_value)
                if match:
                    doc["total_floors_count"] = match.group(2)
                    break
    if doc["total_floors_count"] == "null" and description:
        match = re.search(r"(\d+)\s*Ø§Ø²\s*(\d+)", description)
        if match:
            doc["total_floors_count"] = match.group(2)
    doc["unit_per_floor"] = next(
        (
            m.get("data", {}).get("value")
            for m in modal_features
            if m.get("data", {}).get("title") == "ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø­Ø¯ Ø¯Ø± Ø·Ø¨Ù‚Ù‡"
        ),
        "null",
    )
    features_map = {
        "Ø¢Ø³Ø§Ù†Ø³ÙˆØ±": "has_elevator",
        "Ù¾Ø§Ø±Ú©ÛŒÙ†Ú¯": "has_parking",
        "Ø§Ù†Ø¨Ø§Ø±ÛŒ": "has_warehouse",
        "Ø¨Ø§Ù„Ú©Ù†": "has_balcony",
        "Ø³Ø±Ù…Ø§ÛŒØ´ Ø¯Ø§Ú©Øª Ø§Ø³Ù¾Ù„ÛŒØª": "has_cooling_system",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø¯Ø§Ú©Øª Ø§Ø³Ù¾Ù„ÛŒØª": "has_heating_system",
        "ØªØ§Ù”Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù… Ù¾Ú©ÛŒØ¬": "has_warm_water_provider",
        "Ø¢Ø¨": "has_water",
        "Ø¨Ø±Ù‚": "has_electricity",
        "Ú¯Ø§Ø²": "has_gas",
        "Ù†Ú¯Ù‡Ø¨Ø§Ù†": "has_security_guard",
        "Ø¨Ø§Ø±Ø¨ÛŒÚ©ÛŒÙˆ": "has_barbecue",
        "Ø§Ø³ØªØ®Ø±": "has_pool",
        "Ø¬Ú©ÙˆØ²ÛŒ": "has_jacuzzi",
        "Ø³ÙˆÙ†Ø§": "has_sauna",
    }
    floor_material_map = {
        "Ø¬Ù†Ø³ Ú©Ù Ø³Ù†Ú¯": "stone",
        "Ø¬Ù†Ø³ Ú©Ù Ø³Ø±Ø§Ù…ÛŒÚ©": "ceramic",
        "Ø¬Ù†Ø³ Ú©Ù Ù…ÙˆÚ©Øª": "carpet",
        "Ø¬Ù†Ø³ Ú©Ù Ù¾Ø§Ø±Ú©Øª Ú†ÙˆØ¨ÛŒ": "wood_parquet",
        "Ø¬Ù†Ø³ Ú©Ù Ù…ÙˆØ²Ø§ÛŒÛŒÚ©": "mosaic",
        "Ø¬Ù†Ø³ Ú©Ù Ù¾Ø§Ø±Ú©Øª Ù„Ù…ÛŒÙ†Øª": "laminate_parquet",
        "Ø¬Ù†Ø³ Ú©Ù Ù¾ÙˆØ´Ø´ Ú©Ù": "floor_covering",
    }
    warm_water_provider_map = {
        "ØªØ§Ù”Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù… Ù¾Ú©ÛŒØ¬": "package",
        "ØªØ§Ù”Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù… Ø¢Ø¨Ú¯Ø±Ù…Ú©Ù†": "water_heater",
        "ØªØ§Ù”Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù… Ù…ÙˆØªÙˆØ±Ø®Ø§Ù†Ù‡": "powerhouse",
    }
    cooling_system_map = {
        "Ø³Ø±Ù…Ø§ÛŒØ´ Ú©ÙˆÙ„Ø± Ú¯Ø§Ø²ÛŒ": "split",
        "Ø³Ø±Ù…Ø§ÛŒØ´ Ú©ÙˆÙ„Ø± Ø¢Ø¨ÛŒ": "water_cooler",
        "Ø³Ø±Ù…Ø§ÛŒØ´ Ø¯Ø§Ú©Øª Ø§Ø³Ù¾Ù„ÛŒØª": "duct_split",
        "Ø³Ø±Ù…Ø§ÛŒØ´ Ø§Ø³Ù¾Ù„ÛŒØª": "split",
        "Ø³Ø±Ù…Ø§ÛŒØ´ ÙÙ† Ú©ÙˆÛŒÙ„": "fan_coil",
        "Ø³Ø±Ù…Ø§ÛŒØ´ Ù‡ÙˆØ§Ø³Ø§Ø²": "air_conditioner",
    }
    heating_system_map = {
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø´ÙˆÙØ§Ú˜": "shoofaj",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø¯Ø§Ú©Øª Ø§Ø³Ù¾Ù„ÛŒØª": "duct_split",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø¨Ø®Ø§Ø±ÛŒ": "heater",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø§Ø³Ù¾Ù„ÛŒØª": "split",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø´ÙˆÙ…ÛŒÙ†Ù‡": "fireplace",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ Ø§Ø² Ú©Ù": "floor_heating",
        "Ú¯Ø±Ù…Ø§ÛŒØ´ ÙÙ† Ú©ÙˆÛŒÙ„": "fan_coil",
    }
    restroom_map = {
        "Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ùˆ ÙØ±Ù†Ú¯ÛŒ": "squat_seat",
        "Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ": "squat",
        "Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ ÙØ±Ù†Ú¯ÛŒ": "seat",
    }
    property_type_map = {
        "ÙˆÛŒÙ„Ø§ÛŒ Ø³Ø§Ø­Ù„ÛŒ": "beach",
        "ÙˆÛŒÙ„Ø§ÛŒ Ø¬Ù†Ú¯Ù„ÛŒ": "jungle",
        "ÙˆÛŒÙ„Ø§ÛŒ Ú©ÙˆÙ‡Ø³ØªØ§Ù†ÛŒ": "mountain",
        "ÙˆÛŒÙ„Ø§ÛŒ Ø¬Ù†Ú¯Ù„ÛŒ-Ú©ÙˆÙ‡Ø³ØªØ§Ù†ÛŒ": "jungle-mountain",
        "Ø³Ø§ÛŒØ±": "other",
    }
    building_direction_map = {
        "Ø´Ù…Ø§Ù„ÛŒ": "north",
        "Ø¬Ù†ÙˆØ¨ÛŒ": "south",
        "Ø´Ø±Ù‚ÛŒ": "east",
        "ØºØ±Ø¨ÛŒ": "west",
        "Ù†Ø§Ù…Ø´Ø®Øµ": "unselect",
    }
    all_feature_fields = [
        "has_balcony",
        "has_elevator",
        "has_warehouse",
        "has_parking",
        "construction_year",
        "is_rebuilt",
        "has_water",
        "has_warm_water_provider",
        "has_electricity",
        "has_gas",
        "has_heating_system",
        "has_cooling_system",
        "has_restroom",
        "has_security_guard",
        "has_barbecue",
        "building_direction",
        "has_pool",
        "has_jacuzzi",
        "has_sauna",
        "floor_material",
        "property_type",
    ]
    for f in all_feature_fields:
        doc[f] = "null"
    if group_feature_row:
        for it in group_feature_row.get("data", {}).get("items", []) or []:
            title = it.get("title", "") or ""
            available = it.get("available")
            for k, v in features_map.items():
                if k in title:
                    if "Ù†Ø¯Ø§Ø±Ø¯" in title:
                        doc[v] = False
                    elif available is not None:
                        doc[v] = bool(available)
                    else:
                        doc[v] = True
    for m in modal_features:
        mdata = m.get("data", {}) or {}
        title = mdata.get("title", "") or mdata.get("text", "") or ""
        for k, v in features_map.items():
            if k in title:
                if "Ù†Ø¯Ø§Ø±Ø¯" in title:
                    doc[v] = False
                else:
                    doc[v] = True
        if m.get("widget_type") == "UNEXPANDABLE_ROW" and title == "ÙˆØ¶Ø¹ÛŒØª ÙˆØ§Ø­Ø¯":
            doc["is_rebuilt"] = mdata.get("value", "null") == "Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡"
        if m.get("widget_type") == "UNEXPANDABLE_ROW" and title == "Ø¬Ù‡Øª Ø³Ø§Ø®ØªÙ…Ø§Ù†":
            doc["building_direction"] = building_direction_map.get(
                mdata.get("value", "unselect"), "unselect"
            )
        if "Ú©Ù" in title:
            doc["floor_material"] = floor_material_map.get(title, "unselect")
        if "ØªØ§Ù”Ù…ÛŒÙ†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¢Ø¨ Ú¯Ø±Ù…" in title:
            doc["has_warm_water_provider"] = warm_water_provider_map.get(
                title, "unselect"
            )
        if "Ø³Ø±Ù…Ø§ÛŒØ´" in title:
            doc["has_cooling_system"] = cooling_system_map.get(title, "unselect")
        if "Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ" in title:
            doc["has_restroom"] = restroom_map.get(title, "unselect")
        if m.get("widget_type") == "FEATURE_ROW" and "Ú¯Ø±Ù…Ø§ÛŒØ´" in title:
            doc["has_heating_system"] = heating_system_map.get(title, "unselect")
    for section in data.get("sections", []):
        if section.get("section_name") == "LIST_DATA":
            for widget in section.get("widgets", []):
                if widget.get("widget_type") == "GROUP_INFO_ROW":
                    for item in widget.get("data", {}).get("items", []):
                        title = item.get("title", "") or ""
                        if title == "Ø³Ø§Ø®Øª":
                            doc["construction_year"] = item.get("value", "null")
                if widget.get("widget_type") == "UNEXPANDABLE_ROW":
                    mdata = widget.get("data", {}) or {}
                    title = mdata.get("title", "") or ""
                    if title == "Ù†ÙˆØ¹ Ù…Ù„Ú©":
                        doc["property_type"] = property_type_map.get(
                            mdata.get("value", ""), "other"
                        )
    doc["regular_person_capacity"] = "null"
    doc["extra_person_capacity"] = "null"
    doc["cost_per_extra_person"] = "null"
    doc["rent_price_on_regular_days"] = "null"
    doc["rent_price_on_special_days"] = "null"
    doc["rent_price_at_weekends"] = "null"
    lat = None
    lon = None
    radius = "null"
    seo_geo = data.get("seo", {}).get("post_seo_schema", {}).get("geo", {}) or {}
    lat = seo_geo.get("latitude") or seo_geo.get("lat") or None
    lon = seo_geo.get("longitude") or seo_geo.get("lng") or seo_geo.get("long") or None
    if not lat or not lon:
        map_section = next(
            (s for s in data.get("sections", []) if s.get("section_name") == "MAP"), {}
        )
        map_widgets = map_section.get("widgets", []) or []
        map_widget = next(
            (w for w in map_widgets if w.get("data", {}).get("location")), None
        )
        if map_widget:
            location = map_widget.get("data", {}).get("location", {}) or {}
            fuzzy = location.get("fuzzy_data") or {}
            exact = location.get("exact_data") or {}
            if fuzzy:
                center = fuzzy.get("point") or fuzzy.get("center") or {}
                lat = center.get("latitude") or center.get("lat") or lat
                lon = center.get("longitude") or center.get("lng") or lon
                radius = fuzzy.get("radius") or fuzzy.get("r") or "null"
            elif exact:
                lat = exact.get("latitude") or exact.get("lat") or lat
                lon = exact.get("longitude") or exact.get("lng") or lon
                radius = "null"
            else:
                radius = location.get("radius", "null")
    doc["location_latitude"] = str(lat) if lat is not None else "null"
    doc["location_longitude"] = str(lon) if lon is not None else "null"
    doc["location_radius"] = radius if radius is not None else "null"
    images = []
    schema_images = data.get("seo", {}).get("post_seo_schema", {}).get("image")
    if isinstance(schema_images, list):
        images.extend([i for i in schema_images if i])
    elif schema_images:
        images.append(schema_images)
    for section in data.get("sections", []) or []:
        if section.get("section_name") == "IMAGE":
            for widget in section.get("widgets", []) or []:
                if widget.get("widget_type") == "IMAGE_CAROUSEL":
                    for item in widget.get("data", {}).get("items", []) or []:
                        img = item.get("image", {}).get("url")
                        if img:
                            images.append(img)
    doc["images"] = list(dict.fromkeys(images))
    return doc

class KafkaMessageSensor(BaseSensorOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.bootstrap_servers = KAFKA_BOOTSTRAP_SERVERS
        self.topic = KAFKA_TOPIC

    def poke(self, context):
        try:
            consumer = KafkaConsumer(
                self.topic,
                bootstrap_servers=self.bootstrap_servers,
                # auto_offset_reset="latest",
                auto_offset_reset="earliest",
                group_id="divar_sensor_group",
                enable_auto_commit=False,
            )
            messages = consumer.poll(timeout_ms=5000)
            has_messages = any(len(records) > 0 for records in messages.values())
            consumer.close()

            if has_messages:
                print(f"âœ… Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ÛŒ Ø¯Ø± ØªØ§Ù¾ÛŒÚ© '{self.topic}' Ù¾ÛŒØ¯Ø§ Ø´Ø¯.")
            else:
                print(f"âš ï¸ Ù‡ÛŒÚ† Ù¾ÛŒØ§Ù…ÛŒ Ø¯Ø± ØªØ§Ù¾ÛŒÚ© '{self.topic}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù…Ù†ØªØ¸Ø± Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù…...")

            return has_messages

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Kafka: {e}")
            return False
    
        #     # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù¾ÛŒØ§Ù…
        #     messages = consumer.poll(timeout_ms=10000)
        #     consumer.close()
        #     return bool(messages.get(self.topic))
        # except Exception as e:
        #     print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§ÙÚ©Ø§: {e}")
        #     return False

def consume_and_fetch(**kwargs):
    consumer = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        group_id="divar_consumer_group",
        value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    )
    messages = consumer.poll(timeout_ms=10000, max_records=1)
    consumer.commit()
    consumer.close()

    for topic_partition, partition_messages in messages.items():
        for message in partition_messages:
            token = message.value
            url = DIVAR_API_URL.format(token)
            try:
                with httpx.Client() as client:
                    response = client.get(url, headers={"User-Agent": USER_AGENT_DEFAULT})
                    response.raise_for_status()
                    data = response.json()
                    kwargs['ti'].xcom_push(key='fetched_data', value=data)
                    kwargs['ti'].xcom_push(key='token', value=token)
                    print(f"Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯: Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† {token}")
                    return
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ {token}: {e}")
                return
    print("Ù‡ÛŒÚ† Ù¾ÛŒØ§Ù…ÛŒ Ø¯Ø± Ú©Ø§ÙÚ©Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

def transform_data(**kwargs):
    data = kwargs['ti'].xcom_pull(key='fetched_data', task_ids='consume_and_fetch')
    token = kwargs['ti'].xcom_pull(key='token', task_ids='consume_and_fetch')
    if not data:
        print(f"Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† {token}.")
        return

    try:
        transformed = transform_json_to_doc(data)
        transformed["post_token"] = token
        transformed["crawl_timestamp"] = datetime.utcnow().isoformat()
        kwargs['ti'].xcom_push(key='transformed_data', value=transformed)
        print(f"ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯: Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† {token}")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ JSON Ø¨Ø±Ø§ÛŒ {token}: {e}")

def store_to_mongo(**kwargs):
    transformed = kwargs['ti'].xcom_pull(key='transformed_data', task_ids='transform_data')
    token = kwargs['ti'].xcom_pull(key='token', task_ids='consume_and_fetch')
    if not transformed:
        print(f"Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± MongoDB ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† {token}.")
        return

    client = MongoClient(MONGO_URI)
    db = client[MONGO_DB]
    collection = db[MONGO_COLLECTION]
    try:
        collection.create_index("post_token", unique=True)
        collection.insert_one(transformed)
        print(f"Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† {token} Ø¯Ø± MongoDB")
    except DuplicateKeyError:
        print(f"ØªÚ©Ø±Ø§Ø±ÛŒ: ØªÙˆÚ©Ù† {token} Ù‚Ø¨Ù„Ø§Ù‹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ {token}: {e}")
    finally:
        client.close()

# --- ØªØ¹Ø±ÛŒÙ DAGÙ‡Ø§ ---
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 10, 8),
    "retries": 5,
    "retry_delay": timedelta(minutes=1),
}

producer_dag = DAG(
    "divar_crawler8",
    default_args=default_args,
    description="Ø§Ø³ØªØ®Ø±Ø§Ø¬ 100 ØªÙˆÚ©Ù† Ø¯ÛŒÙˆØ§Ø±ØŒ ÙÛŒÙ„ØªØ± Ø¨Ø§ Ø¨Ù„ÙˆÙ…ØŒ Ùˆ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ú©Ø§ÙÚ©Ø§ Ù‡Ø± 5 Ø¯Ù‚ÛŒÙ‚Ù‡",
    schedule_interval="*/5 * * * *",
    catchup=False,
)

consumer_dag = DAG(
    "divar_fetch8",
    default_args=default_args,
    description="Ù…ØµØ±Ù ÛŒÚ© ØªÙˆÚ©Ù† Ø§Ø² Ú©Ø§ÙÚ©Ø§ØŒ Ø¯Ø±ÛŒØ§ÙØªØŒ ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± MongoDB Ù‡Ø± 5 Ø¯Ù‚ÛŒÙ‚Ù‡",
    schedule_interval="*/5 * * * *",
    catchup=False,
)

# --- ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ DAG ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ù‡ ---
extract_task = PythonOperator(
    task_id="extract_tokens",
    python_callable=extract_tokens,
    provide_context=True,
    dag=producer_dag,
)

filter_task = PythonOperator(
    task_id="filter_tokens",
    python_callable=filter_tokens,
    provide_context=True,
    dag=producer_dag,
)

produce_task = PythonOperator(
    task_id="produce_to_kafka",
    python_callable=produce_to_kafka,
    provide_context=True,
    dag=producer_dag,
)

#  Ú¯Ø±Ø§Ù DAG ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ù‡
extract_task >> filter_task >> produce_task

# --- ØªØ³Ú©â€ŒÙ‡Ø§ÛŒ DAG Ù…ØµØ±Ùâ€ŒÚ©Ù†Ù†Ø¯Ù‡ ---
kafka_sensor = KafkaMessageSensor(
    task_id="kafka_message_sensor",
    poke_interval=60,
    timeout=600,
    dag=consumer_dag,
)

consume_fetch_task = PythonOperator(
    task_id="consume_and_fetch",
    python_callable=consume_and_fetch,
    provide_context=True,
    dag=consumer_dag,
)

transform_task = PythonOperator(
    task_id="transform_data",
    python_callable=transform_data,
    provide_context=True,
    dag=consumer_dag,
)

store_task = PythonOperator(
    task_id="store_to_mongo",
    python_callable=store_to_mongo,
    provide_context=True,
    dag=consumer_dag,
)

#  Ú¯Ø±Ø§Ù DAG Ù…ØµØ±Ùâ€ŒÚ©Ù†Ù†Ø¯Ù‡
kafka_sensor >> consume_fetch_task >> transform_task >> store_task